---
title: "8의 배수의 힘: 텐서 코어의 최대 활용 (행렬 곱셈의 처리량 극대화)"
tags:
    - GPU
    - Tensor Cores
    - CUDA
    - Deep Learning
    - Optimization
    - Performance
    - NVIDIA
    - Parallel Computing
date: "2026-02-15"
thumbnail: "/assets/img/thumbnail/tensor_core.png"
bookmark: true
---
모든 코드는 [why-multiple-of-8](https://github.com/codingbus821/why-multiple-of-8) 에 제공되어있습니다.

# Introduction

Matrix multiplications은 대부분의 딥러닝 아키텍처에서 핵심적인 역할을 합니다. 대규모의 훈련이나 추론 시 매 forward pass마다 수행되는 matrix multiplications의 횟수를 고려해 보면 이 연산의 성능을 극대화하는 방법은 상당한 속도 향상을 가져올 수 있습니다. 행렬의 차원을 정렬하는 것이 이 연산의 처리량을 극대화하는 데 매우 중요합니다.

Nvidia GPU에는 `Tensor Cores`라는 특정한 하드웨어 구성 요소가 있으며, 이는 오직 matrix multiplication을 위해서 사용됩니다. `Tensor Cores`는 `Float16`, `BFloat16`, `TF32`라는 특정한 데이터 유형을 요구하며 작은 `blocks` 단위로 행렬을 처리합니다. 아래 A100의 사양은 이를 잘 보여줍니다. `FP16`또는 `BFloat16`로 matrix multiplication을 실행하면 최대 312TFLOPS를 달성할 수 있지만 Tensor Cores를 사용할 수 없는 `FP32` non matrix multiplication은 최대 19.5TFLOPS에 불과합니다.

![](https://github-production-user-asset-6210df.s3.amazonaws.com/42774231/550157516-e5decdbd-8801-4132-9251-5e165456230b.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20260215%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20260215T145039Z&X-Amz-Expires=300&X-Amz-Signature=5c6bca702aeb6c7fe192acb4e95a476fa7806d9d9f977e89b47e714bb8e1cf47&X-Amz-SignedHeaders=host)
[이미지 출처](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf)


Nvidia cuBLAS와 같은 라이브러리의 커널은 매우 정교하게 튜닝되어 있습니다. 하지만 실제 데이터는 레지스터에 로드되고 다른 하드웨어로 이동된 후 GPU 메인 메모리에 다시 저장되더야 하기 때문에 이 최대 수치를 달성하는 것은 사실상 불가능합니다.

[PyTorch Performance Tuning Guide](https://docs.pytorch.org/tutorials/recipes/recipes/tuning_guide.html#use-mixed-precision-and-amp)를 살펴보면 mixed precision 섹션에 적혀있는 바로는

> To use Tensor Cores:
> 
> - **set sizes to multiples of 8** (to map onto dimensions of Tensor Cores)
>     - see [Deep Learning Performance Documentation](https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance) for more details and guidelines specific to layer type
>     - if layer size is derived from other parameters rather than fixed, it can still be explicitly padded e.g. vocabulary size in NLP models
> - enable AMP
>     - Introduction to Mixed Precision Training and AMP: [slides](https://nvlabs.github.io/eccv2020-mixed-precision-tutorial/files/dusan_stosic-training-neural-networks-with-tensor-cores.pdf)
>     - native PyTorch AMP is available: [documentation](https://pytorch.org/docs/stable/amp.html), [examples](https://pytorch.org/docs/stable/notes/amp_examples.html#amp-examples), [tutorial](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html)
- **set sizes to multiples of 8**

이 적혀있는 것을 확인할 수 있습니다. 그러나 왜 8일까요?

구 버전의 Nvidia cuBLAS 또는 cuDNN에서는 `tensor cores`를 사용하기 위해 행렬 차원이 16 bytes의 배수여야 하는 요구 사항이 있었습니다. `FP16`과 `BFloat16`은 2 bytes이므로, 이는 행렬 차원이 8의 배수여야 함을 의미합니다. 최신 버전의 cuBLAS 및 cuDNN에서는 이 엄격한 요구사항이 완화되어 8의 배수가 아니어도 tensor cores를 사용할 수 있지만, 여전히 행렬 차원이 8의 배수일 때 가장 성능이 좋습니다. 자세한 내용은 [여기](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc)의 Nvidia 문서를 참고하세요.

벤치마크를 실행해 보고 얼마나 영향을 미치는지 알아봅시다.

# **Padding to multiples of 8**

두 개의 행렬 `A`, `B`가 주어지고 각각 차원이 `(M, K)`, `(K, M)`인 경우, `M`을 고정하고 `K`를 증가시키면서 `A@B` (A와 B의 행렬 곱)를 벤치마킹해 보겠습니다. K는 8의 배수에서 벗어난 정수이므로, 이 행렬들의 내부 차원을 8의 배수가 되도록 채워주는 함수가 필요합니다. 아래와 같이 PyTorch로 비교적 간단하게 작성할 수 있습니다.

```python
import torch
import numpy as np

def pad_by_8(in_tensor: torch.tensor, axis: int) -> torch.tensor:
  """Pads the specified axis of the tensor to be the nearest
  multiple of 8 that is larger than it's current size.

  Args:
    in_tensor (torch.tensor): The tensor to pad
    axis: (int): the axis to pad on

  Returns:
    torch.tensor: the padded tensor
  """
  if axis < 0 or axis >= in_tensor.ndim:
      raise ValueError(f"Axis {axis} is out of bounds for tensor with shape {in_tensor.shape}")

  current_size = in_tensor.size(axis)
  padding_needed = 8 - (current_size % 8)
  padding = [0] * (2 * in_tensor.ndim)
  padding[2 * axis + 1] = padding_needed

  # Reverse the padding tuple because PyTorch expects padding for last dimension first
  padding = tuple(padding[::-1])

  # Pad the tensor
  return torch.nn.functional.pad(in_tensor, padding, "constant", 0)

print("=== BEFORE PADDING ===")
A = np.arange(10).reshape(2,5)
B = np.arange(10).reshape(5,2)
print("A shape=", A.shape)
print(A)
print("-" * 50)
print("B shape=", B.shape)
print(B)
print("-" * 50)
print("A@B shape=", (A@B).shape)
print(A@B)
print("-" * 50)

print()
print("=== AFTER PADDING ===")
pad_A = pad_by_8(torch.from_numpy(A), 1)
pad_B = pad_by_8(torch.from_numpy(B), 0)

print("pad_A shape=", pad_A.shape)
print(pad_A)
print("-" * 50)
print("pad_B shape=", pad_B.shape)
print(pad_B)
print("-" * 50)
print("pad_A@pad_B shape=", (pad_A@pad_B).shape)
print(pad_A@pad_B)
print("-" * 50)
```

## 출력:

```python
=== BEFORE PADDING ===
A shape= (2, 5)
[[0 1 2 3 4]
 [5 6 7 8 9]]
--------------------------------------------------
B shape= (5, 2)
[[0 1]
 [2 3]
 [4 5]
 [6 7]
 [8 9]]
--------------------------------------------------
A@B shape= (2, 2)
[[ 60  70]
 [160 195]]
--------------------------------------------------

=== AFTER PADDING ===
pad_A shape= torch.Size([2, 8])
tensor([[0, 0, 0, 0, 1, 2, 3, 4],
        [0, 0, 0, 5, 6, 7, 8, 9]])
--------------------------------------------------
pad_B shape= torch.Size([8, 2])
tensor([[0, 0],
        [0, 0],
        [0, 0],
        [0, 1],
        [2, 3],
        [4, 5],
        [6, 7],
        [8, 9]])
--------------------------------------------------
pad_A@pad_B shape= torch.Size([2, 2])
tensor([[ 60,  70],
        [160, 195]])
--------------------------------------------------
```

그리고 `pad_A = pad_by_8(A, axis=1)`과 `pad_B = pad_by_8(B, axis=0)`을 생성합니다. 패딩은 최종 결과에 영향을 주지 않는 0을 추가하기 때문에 `pad_A@pad_B==A@B`임을 쉽게 확인할 수 있습니다.

# Benchmarking

[Triton](https://triton-lang.org/main/index.html) 의 벤치마킹 유틸리티를 사용합니다. 매 반복마다, `K`는 8의 배수에서 1과 7사이의 임의의 정수를 더하여 줍니다. 두 행렬 `A, B`는 이 값으로 생성된 후 8의 배수가 되도록 패딩 처리된 버전도 함께 생성됩니다. 행렬 곱셈이 수행되고 `M, K` 및 `duration`을 기반으로 TFLOPs/s가 계산됩니다.

```python
import torch
import triton

configs = [triton.testing.Benchmark(
            x_names=["K"],
            x_vals=[8 * i for i in range(16, 2000, 10)],
            line_arg="method",
            line_vals=["padded", "normal"],
            line_names=["padded", "normal"],
            styles=[("green", "-"), ("blue", "-")],
            ylabel="TFLOPS",
            plot_name="benchmark",
            args={"M": 256}
)]

@triton.testing.perf_report(configs)
def benchmark_matmul(K, method, M):
    K = K + torch.randint(1, 8, (1,)).item()
    A = torch.rand((M, K), device="cuda", dtype=torch.bfloat16)
    B = torch.rand((K, M), device="cuda", dtype=torch.bfloat16)
    pad_A = pad_by_8(A, axis=1)
    pad_B = pad_by_8(B, axis=0)

    quantiles = [0.5, 0.2, 0.8]
    if method == "normal":
      ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.mm(A, B), quantiles=quantiles)
    if method == "padded":
      ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.mm(pad_A, pad_B), quantiles=quantiles)

    perf = lambda ms: (2*K*M*M) * 1e-12 / (ms * 1e-3)
    return perf(ms), perf(max_ms), perf(min_ms)

benchmark_matmul.run(show_plots=True, print_data=False) # set print_data = True to see a table of output
```

결과는 모두 4060 Ti 8GB에서 실행됐습니다.

`M=256`일 때 다음과 같은 그래프가 생성됩니다.

![Benchmark results with M = 256](https://github-production-user-asset-6210df.s3.amazonaws.com/42774231/550161845-77d13b83-bee9-4a78-b55e-ed8b28b3f895.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20260215%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20260215T150211Z&X-Amz-Expires=300&X-Amz-Signature=a89c89e8e2c7507f1dd6dd9cd9a236080c3c4cfc4f1c34c7f320295791c38527&X-Amz-SignedHeaders=host)
[Benchmark results with M = 256]

패딩을 했을 때 성능이 개선되었습니다.

![출처:https://www.nvidia.com/ko-kr/geforce/graphics-cards/40-series/rtx-4060-4060ti/](https://github-production-user-asset-6210df.s3.amazonaws.com/42774231/550162683-55f44754-64b4-4d88-88be-7ee183d1e3f3.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20260215%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20260215T150422Z&X-Amz-Expires=300&X-Amz-Signature=ee04dbf489e8a82269f467a5ccab34480f07a212ea82b4229b48589a410595fe&X-Amz-SignedHeaders=host)

[이미지 출처](https://www.nvidia.com/ko-kr/geforce/graphics-cards/40-series/rtx-4060-4060ti/)

그러나 이는 약 25TFLOPS에 불과하며 4060 Ti의 스펙상 성능인 51TFLOPS에 훨씬 못 미칩니다. TFLOPs/s는 `M^2`에 비례하므로 `M`을 1024로 증가시키면 더 높은 TFLOPs/s를 기대할 수 있습니다.

![Benchmark results for M = 1024](https://github-production-user-asset-6210df.s3.amazonaws.com/42774231/550164837-0228e917-f004-424b-a216-6091ad1241af.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20260215%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20260215T151016Z&X-Amz-Expires=300&X-Amz-Signature=c912e630afe0a6065930d6fd01b5b6ebde5337f3b6c2271b8c2c7758e541f2a3&X-Amz-SignedHeaders=host)
[Benchmark results for M = 1024]

패딩 처리된 버전은 약 45 TFLOPs/s까지 도달하지만 일반 버전은 그에 못 미칩니다. `K`가 증가함에 따라 여전히 증가하고 있으므로 `M`을 더 늘리면 TFLOPs/s도 더 증가할 것입니다.

`M=4096`일 때 다음과 같은 그래프가 생성됩니다.

![Benchmark results with M = 4096](https://github-production-user-asset-6210df.s3.amazonaws.com/42774231/550165055-9fc06ba8-c3ed-4334-b516-921711d0b072.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20260215%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20260215T151107Z&X-Amz-Expires=300&X-Amz-Signature=d2a3751160dcf024f9e3e893b35813ceffb09de155970dd440f7c283ed5e6fe1&X-Amz-SignedHeaders=host)
[Benchmark results with M = 4096]

그러면 inner dimension을 변경하는 대신 outer dimension을 변경하면 어떨까요? 즉 `(M, K)@(K, M)` 대신 `M`을 고정하고 `K`를 증가시키면서 `(K, M)@(M, K)`를 벤치마킹합니다. 이는 위의 벤치마킹 코드에서 약간의 수정만 하면 됩니다. 결과는 매우 유사한 패턴을 보여줍니다. 예를 들어 `M=1024`일 때 8의 배수로 `K`를 패딩 하는 것이 더 효율적입니다.

```python
import torch
import triton

configs = [triton.testing.Benchmark(
            x_names=["K"],
            x_vals=[8 * i for i in range(16, 2000, 10)],
            line_arg="method",
            line_vals=["padded", "normal"],
            line_names=["padded", "normal"],
            styles=[("green", "-"), ("blue", "-")],
            ylabel="TFLOPS",
            plot_name="benchmark",
            args={"M": 1024}
)]

@triton.testing.perf_report(configs)
def benchmark_matmul(K, method, M):
    K = K + torch.randint(1, 8, (1,)).item()
    A = torch.rand((K, M), device="cuda", dtype=torch.bfloat16)
    B = torch.rand((M, K), device="cuda", dtype=torch.bfloat16)
    pad_A = pad_by_8(A, axis=0)
    pad_B = pad_by_8(B, axis=1)

    quantiles = [0.5, 0.2, 0.8]
    if method == "normal":
      ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.mm(A, B), quantiles=quantiles)
    if method == "padded":
      ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.mm(pad_A, pad_B), quantiles=quantiles)

    perf = lambda ms: (2*K*K*M) * 1e-12 / (ms * 1e-3)
    return perf(ms), perf(max_ms), perf(min_ms)

benchmark_matmul.run(show_plots=True, print_data=False) # set print_data = True to see a table of output
```

![Benchmark results with M = 1024](https://github-production-user-asset-6210df.s3.amazonaws.com/42774231/550165207-5b3b67e8-2643-4583-a022-47cd6ba72700.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20260215%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20260215T151148Z&X-Amz-Expires=300&X-Amz-Signature=c6bd78c54c0e2a642e77122a3ac3c2c065d10e790f25f4eab67bf910f96f0893&X-Amz-SignedHeaders=host)
[Benchmark results with M = 1024]

# Conclusion

불규칙한(8의 배수가 아닌) 행렬 크기는 GPU를 최적으로 활용할 가능성이 적습니다. Matrix multiplications는 흔히 이미 최적화된 연산이라고 여겨지지만 성능을 쉽게 낭비할 수 있습니다. 파라미터의 차원이나 입력 크기(e.g. vocab sizes, 3D mesh data)는 이러한 결과를 고려하여 선택하거나, `Tensor Cores`를 최대한 활용하기 위해 미리 8의 배수로 맞추도록 패딩 처리할 수 있습니다.

# 5090과의 비교

아래는 5090 32GB의 결과입니다.

`M=256`

![Benchmark results with M = 256](https://github-production-user-asset-6210df.s3.amazonaws.com/42774231/550165342-2bc4d829-d7d7-4bea-a281-d8dbaecbd0ee.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20260215%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20260215T151226Z&X-Amz-Expires=300&X-Amz-Signature=50172c96aea3260af44ef1c2486100b6a62c1d54d954224367952536ed06dd77&X-Amz-SignedHeaders=host)
[Benchmark results with M = 256]

`M=1024`

![Benchmark results with M = 1024](https://github-production-user-asset-6210df.s3.amazonaws.com/42774231/550165453-c00e2e93-ca5e-472d-83b0-f2cce3a57e4a.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20260215%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20260215T151257Z&X-Amz-Expires=300&X-Amz-Signature=d467634f4393cfe117e58ca091575d17fb39da75f44d462d3840fc0430a5599d&X-Amz-SignedHeaders=host)
[Benchmark results with M = 1024]

`M=4096`

![Benchmark results with M = 4096](https://github-production-user-asset-6210df.s3.amazonaws.com/42774231/550165597-d15ca172-26d4-4716-8ca9-556fc44f3a8b.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20260215%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20260215T151332Z&X-Amz-Expires=300&X-Amz-Signature=0103f050ba1c32d2cea5cc0a62b15c129cfccdf85f506497060ea251e116996c&X-Amz-SignedHeaders=host)
[Benchmark results with M = 4096]

이 글의 원문인 아래 참고 자료와 비교했을 때 A100 → 4060 Ti → 5090 으로 갈수록 normal에서 여전히 불안정하지만 padded와의 격차가 줄어드는 것을 볼 수 있습니다. 이것은 아마 Tensor Core 구조가 Ampere (A100) → Ada (4060Ti) → Blackwell (5090)으로 진화하면서 개선된 결과이지 않을까 생각합니다. 더 자세히 알고 계시는 분은 댓글 달아주시면 감사하겠습니다.

---

참고자료
<https://medium.com/@michael.diggin/the-power-of-8-getting-the-most-out-of-tensor-cores-c7704ae0c5c1>